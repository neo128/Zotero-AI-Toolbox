# Awesome-VLA-Papers

This repository contains the list of representative VLA works in the survey â€œ[*A Survey on Vision-Language-Action Models: An Action Tokenization Perspective*](https://arxiv.org/abs/2507.01925)â€, along with relevant reference materials.

# Foundation Models

## Language Foundation Models

- **Transformer**, *Attention is All You Need*, 2017.06, NIPS 2017. [[ğŸ“„ Paper](https://arxiv.org/abs/1706.03762)]
- **USE**, *Universal sentence encoder*, 2018.03. [[ğŸ“„ Paper](https://arxiv.org/abs/1803.11175)]
- **GPT-1**, *Improving language understanding by generative pre-training*, 2018.06. [[ğŸ“„ Paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)]
- **BERT**, *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*, 2018.10, NAACL 2019. [[ğŸ“„ Paper](https://arxiv.org/abs/1810.04805)] [[ğŸ’» Code](https://github.com/google-research/bert)] [[ğŸ¤— Model](https://huggingface.co/google-bert)]
- **GPT-2**, *Language Models are Unsupervised Multitask Learners*, 2019.02. [[ğŸ“„ Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)] [[ğŸ’» Code](https://github.com/openai/gpt-2)]
- **MUSE**, *Multilingual universal sentence encoder for semantic retrieval*, 2019.07. [[ğŸ“„ Paper](https://arxiv.org/abs/1907.04307)] [[ğŸ’» Code](https://github.com/facebookresearch/MUSE)]
- **T5**, *Exploring the limits of transfer learning with a unified text-to-text transformer*, 2019.10, JMLR 2020. [[ğŸ“„ Paper](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)] [[ğŸ’» Code](https://github.com/google-research/text-to-text-transfer-transformer)] [[ğŸ¤— Model](https://huggingface.co/google-t5)]
- **GPT-3**, *Language Models are Few-Shot Learners*, 2020.05, NeurIPS 2020. [[ğŸ“„ Paper](https://arxiv.org/abs/2005.14165)]
- **InstructGPT**, *Training language models to follow instructions with human feedback*, 2022.03, NeurIPS 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2203.02155)] [[ğŸŒ Website](https://openai.com/index/instruction-following/)]
- **Chinchilla**, *Training Compute-Optimal Large Language Models*, 2022.03, NeurIPS 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2203.15556)]
- **ChatGPT**, 2022.11. [[ğŸŒ Website](https://openai.com/index/chatgpt/?utm_source=chatgpt.com)]
- **LLaMA**, *LLaMA: Open and Efficient Foundation Language Models*, 2023.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2302.13971)] [[ğŸ’» Code](https://github.com/meta-llama/llama)] [[ğŸ¤— Model](https://huggingface.co/meta-llama)]
- **GPT-4**, 2023.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2303.08774)] [[ğŸŒ Website](https://openai.com/index/gpt-4-research/)]
- **Claude**, 2023.03. [[ğŸŒ Website](https://www.anthropic.com/news/introducing-claude)]
- **Llama 2**, *Llama 2: Open Foundation and Fine-Tuned Chat Models*, 2023.07. [[ğŸ“„ Paper](https://arxiv.org/abs/2307.09288)] [[ğŸ¤— Model](https://huggingface.co/meta-llama)]
- **Claude 2**, 2023.07. [[ğŸŒ Website](https://www.anthropic.com/news/claude-2)]
- **Mistral**, *Mistral 7B*, 2023.10. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.06825)] [[ğŸ¤— Model](https://huggingface.co/mistralai)]
- **Mamba**, *Mamba: Linear-Time Sequence Modeling with Selective State Spaces*, 2023.12, COML. [[ğŸ“„ Paper](https://arxiv.org/abs/2312.00752)] [[ğŸ’» Code](https://github.com/state-spaces/mamba)]
- **Mixtral**, *Mixtral of Experts*, 2024.01. [[ğŸ“„ Paper](https://arxiv.org/abs/2401.04088)] [[ğŸ¤— Model](https://huggingface.co/mistralai)]
- **Gemma**, *Gemma: Open Models Based on Gemini Research and Technology*, 2024.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.08295)] [[ğŸŒ Website](https://deepmind.google/models/gemma/)]
- **Claude 3**, 2024.03. [[ğŸŒ Website](https://www.anthropic.com/news/claude-3-family)]
- **Llama 3**, *The Llama 3 Herd of Models*, 2024.07. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.21783)] [[ğŸŒ Website](https://www.llama.com/models/llama-3/)] [[ğŸ¤— Model](https://huggingface.co/meta-llama)]
- **Gemma 2**, *Gemma 2: Improving Open Language Models at a Practical Size*, 2024.08. [[ğŸ“„ Paper](https://arxiv.org/abs/2408.00118)] [[ğŸ¤— Model](https://huggingface.co/google/gemma-2-2b-it)]
- **OpenAI o1**, 2024.12. [[ğŸŒ Website](https://openai.com/o1/)]
- **Gemini 2.0 Flash**, 2025.01. [[ğŸŒ Website](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)]
- **DeepSeek-R1**, *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*, 2025.01. [[ğŸ“„ Paper](https://arxiv.org/abs/2501.12948)] [[ğŸ¤— Model](https://huggingface.co/deepseek-ai/DeepSeek-R1)]
- **Gemini 2.0 Pro**, 2025.02. [[ğŸŒ Website](https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/)]
- **Gemini 2.5 Pro**, 2025.03. [[ğŸŒ Website](https://deepmind.google/models/gemini/pro/)]
- **Gemma 3**, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.19786)] [[ğŸŒ Website](https://deepmind.google/models/gemma/gemma-3/)]
- **Gemini 2.5 Flash**, 2025.04. [[ğŸŒ Website](https://deepmind.google/models/gemini/flash/)]
- **Claude 4**, 2024.05. [[ğŸŒ Website](https://www.anthropic.com/news/claude-4)]

## Vision Foundation Models

- **ViT**, *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*, 2020.10, ICLR 2021. [[ğŸ“„ Paper](https://arxiv.org/abs/2010.11929)] [[ğŸ’» Code](https://github.com/google-research/vision_transformer)]
- **CLIP**, *Learning Transferable Visual Models From Natural Language Supervision*, 2021.02, ICML 2021. [[ğŸ“„ Paper](https://arxiv.org/abs/2103.00020)] [[ğŸ’» Code](https://github.com/OpenAI/CLIP)]
- **DINO**, *Emerging Properties in Self-Supervised Vision Transformers*, 2021.04, ICCV 2021. [[ğŸ“„ Paper](https://arxiv.org/abs/2104.14294)] [[ğŸ’» Code](https://github.com/facebookresearch/dino)]
- **GLIP**, *Grounded Language-Image Pre-training*, 2021.12, CVPR 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2112.03857)] [[ğŸ’» Code](https://github.com/microsoft/GLIP)]
- **GLIDE**, *GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models*, 2021.12, ICML 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2112.10741)] [[ğŸ’» Code](https://github.com/openai/glide-text2im)]
- **Stable Diffusion**, *High-Resolution Image Synthesis with Latent Diffusion Models*, 2021.12, CVPR 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2112.10752)] [[ğŸ’» Code](https://github.com/CompVis/latent-diffusion)]
- **DALL-E 2**, *Hierarchical Text-Conditional Image Generation with CLIP Latents*, 2022.04, CVPR 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2204.06125)] [[ğŸŒ Website](https://openai.com/index/dall-e-2/)]
- **Imagen**, *Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding*, 2022.05, NeurIPS 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2205.11487)] [[ğŸŒ Website](https://imagen.research.google/)]
- **Stable Diffusion 2**, 2022.11. [[ğŸŒ Website](https://github.com/Stability-AI/StableDiffusion)]
- **ControlNet**, *Adding Conditional Control to Text-to-Image Diffusion Models*, 2023.02, ICCV 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2302.05543)] [[ğŸ’» Code](https://github.com/lllyasviel/ControlNet)]
- **PVDM**, *Video Probabilistic Diffusion Models in Projected Latent Space*, 2023.02, CVPR 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2302.07685)] [[ğŸŒ Website](https://sihyun.me/PVDM/)] [[ğŸ’» Code](https://github.com/sihyun-yu/PVDM)]
- **SigLIP**, *Sigmoid Loss for Language Image Pre-Training*, 2023.03, ICCV 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2303.15343)] [[ğŸ’» Code](https://github.com/google-research/big_vision)]
- **Grounding DINO**, *Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection*, 2023.03, ECCV 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2303.05499)] [[ğŸ’» Code](https://github.com/IDEA-Research/GroundingDINO)]
- **DINOv2**, *DINOv2: Learning Robust Visual Features without Supervision*, 2023.04, TMLR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2304.07193)] [[ğŸŒ Website](https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/)] [[ğŸ’» Code](https://github.com/facebookresearch/dinov2)]
- **SAM**, *Segment Anything*, 2023.04, ICCV 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2304.02643)] [[ğŸŒ Website](https://segment-anything.com/)] [[ğŸ’» Code](https://github.com/facebookresearch/segment-anything)] [[ğŸ“Š Dataset](https://segment-anything.com/dataset/index.html)]
- **CoTracker**, *CoTracker: It is Better to Track Together*, 2023.07, ECCV 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2307.07635)] [[ğŸŒ Website](https://co-tracker.github.io/)] [[ğŸ’» Code](https://github.com/facebookresearch/co-tracker)]
- **Cutie**, *Putting the Object Back into Video Object Segmentation*, 2023.10, CVPR 2024 Highlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.12982)] [[ğŸŒ Website](https://hkchengrex.com/Cutie/)] [[ğŸ’» Code](https://github.com/hkchengrex/Cutie)]
- **VideoCrafter1**, *VideoCrafter1: Open Diffusion Models for High-Quality Video Generation*, 2023.10. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.19512)] [[ğŸŒ Website](https://ailab-cvc.github.io/videocrafter1/)] [[ğŸ’» Code](https://github.com/AILab-CVC/VideoCrafter)]
- **FoundationPose**, *FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects*, 2023.12, CVPR 2024 Highlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2312.08344)] [[ğŸŒ Website](https://nvlabs.github.io/FoundationPose/)] [[ğŸ’» Code](https://github.com/NVlabs/FoundationPose)]
- **HaMeR**, *Reconstructing Hands in 3D with Transformers*, 2023.12, CVPR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2312.05251)] [[ğŸŒ Website](https://geopavlakos.github.io/hamer/)] [[ğŸ’» Code](https://github.com/geopavlakos/hamer)] [[ğŸ“Š Dataset](https://github.com/ddshan/hint)]
- **Depth Anything**, *Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data*, 2024.01, CVPR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2401.10891)] [[ğŸŒ Website](https://depth-anything.github.io/)] [[ğŸ’» Code](https://github.com/LiheYoung/Depth-Anything)]
- **Grounded SAM**, *Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks*, 2024.01. [[ğŸ“„ Paper](https://arxiv.org/abs/2401.14159)] [[ğŸ’» Code](https://github.com/IDEA-Research/Grounded-Segment-Anything)]
- **VideoCrafter2**, *VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models*, 2024.01, CVPR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2401.09047)] [[ğŸŒ Website](https://ailab-cvc.github.io/videocrafter2/)] [[ğŸ’» Code](https://github.com/AILab-CVC/VideoCrafter)]
- **Sora**, *Video generation models as world simulators*, 2024.02. [[ğŸŒ Website](https://openai.com/index/video-generation-models-as-world-simulators/)]
- **Genie**, *Genie: Generative Interactive Environments*, 2024.02, ICML 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2402.15391)] [[ğŸŒ Website](https://sites.google.com/view/genie-2024/home)]
- **Stable Diffusion 3**, *Scaling Rectified Flow Transformers for High-Resolution Image Synthesis*, 2024.03, ICML 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.03206)] [[ğŸ¤— Model](https://huggingface.co/stabilityai/stable-diffusion-3-medium)]
- **Grounding DINO 1.5**, *Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection*, 2024.05. [[ğŸ“„ Paper](https://arxiv.org/abs/2405.10300)] [[ğŸ’» Code](https://github.com/IDEA-Research/Grounding-DINO-1.5-API)]
- **Depth Anything V2**, *Depth Anything V2*, 2024.06, NeurIPS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2406.09414)] [[ğŸŒ Website](https://depth-anything-v2.github.io/)] [[ğŸ’» Code](https://github.com/DepthAnything/Depth-Anything-V2)]
- **SAM 2**, *SAM 2: Segment Anything in Images and Videos*, 2024.08. [[ğŸ“„ Paper](https://arxiv.org/abs/2408.00714)] [[ğŸŒ Website](https://ai.meta.com/sam2/)] [[ğŸ’» Code](https://github.com/facebookresearch/sam2)]
- **Grounded SAM 2**, *Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks*, 2024.08. [[ğŸ“„ Paper](https://arxiv.org/abs/2401.14159)] [[ğŸ’» Code](https://github.com/IDEA-Research/Grounded-SAM-2)]
- **SAMURAI**, *SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory*, 2024.11. [[ğŸ“„ Paper](https://arxiv.org/abs/2411.11922)] [[ğŸŒ Website](https://yangchris11.github.io/samurai/)] [[ğŸ’» Code](https://github.com/yangchris11/samurai)]
- **Genie 2**, *Genie 2: A large-scale foundation world model*, 2024.11. [[ğŸŒ Website](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)]
- **Veo 3**, *Veo 3*, 2025.05. [[ğŸŒ Website](https://deepmind.google/models/veo/)]

## Vision Language Models

- **BLIP**, *BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation*, 2022.01, ICML 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2201.12086)] [[ğŸ’» Code](https://github.com/salesforce/BLIP)] [[ğŸ¤— Model](https://huggingface.co/collections/Salesforce/blip-models-65242f40f1491fbf6a9e9472)]
- **Flamingo**, *Flamingo: a Visual Language Model for Few-Shot Learning*, 2022.04, NeurIPS 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2204.14198)]
- **BLIP-2**, *BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models*, 2023.01, ICML 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2301.12597)] [[ğŸ¤— Model](https://huggingface.co/collections/Salesforce/blip2-models-65242f91b4c4b4a32e5cb652)]
- **LLaVA**, *Visual Instruction Tuning*, 2023.04, NeurIPS 2023 Oral. [[ğŸ“„ Paper](https://arxiv.org/abs/2304.08485)] [[ğŸŒ Website](https://llava-vl.github.io/)] [[ğŸ’» Code](https://github.com/haotian-liu/LLaVA)] [[ğŸ“Š Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)]
- **Qwen-VL**, *Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond*, 2023.08. [[ğŸ“„ Paper](https://arxiv.org/abs/2308.12966)] [[ğŸ’» Code](https://github.com/QwenLM/Qwen-VL)] [[ğŸ¤— Model](https://huggingface.co/Qwen/Qwen-VL)]
- **LLaVA 1.5**, *Improved Baselines with Visual Instruction Tuning*, 2023.10, CVPR 2024 highlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.03744)] [[ğŸŒ Website](https://llava-vl.github.io/)] [[ğŸ’» Code](https://github.com/haotian-liu/LLaVA)] [[ğŸ¤— Model](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)] [[ğŸ“Š Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)]
- **Prismatic**, *Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models*, 2024.02, ICML 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2402.07865)] [[ğŸ’» Code (training)](https://github.com/TRI-ML/prismatic-vlms)] [[ğŸ’» Code (evaluation)](https://github.com/TRI-ML/vlm-evaluation)] [[ğŸ¤— Model](https://huggingface.co/TRI-ML/prismatic-vlms)]
- **GPT-4o**, 2024.05. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.21276)] [[ğŸŒ Website](https://openai.com/index/hello-gpt-4o/)]
- **PaliGemma**, *PaliGemma: A versatile 3B VLM for transfer*, 2024.07. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.07726)] [[ğŸŒ Website](https://huggingface.co/blog/paligemma)]
- **Qwen2-VL**, *Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution*, 2024.09. [[ğŸ“„ Paper](https://arxiv.org/abs/2409.12191)] [[ğŸŒ Website](https://qwenlm.github.io/blog/qwen2-vl/)] [[ğŸ¤— Model](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)]
- **Qwen2.5-VL**, 2025.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.13923)] [[ğŸŒ Website](https://qwenlm.github.io/blog/qwen2.5-vl/)] [[ğŸ’» Code](https://github.com/QwenLM/Qwen2.5-VL)] [[ğŸ¤— Model](https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5)]
- **Gemini 2.5 Pro**, 2025.03. [[ğŸŒ Website](https://deepmind.google/models/gemini/pro/)]
- **Gemini 2.5 Flash**, 2025.04. [[ğŸŒ Website](https://deepmind.google/models/gemini/flash/)]

# Language Description as Action Tokens

## Language Plan

- **Language Planner**, *Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents*, 2022.01, ICML 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2201.07207)] [[ğŸŒ Website](https://wenlong.page/language-planner/)] [[ğŸ’» Code](https://github.com/huangwl18/language-planner)]
- **Socratic Models**, *Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language*, 2022.04, ICLR 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2204.00598)] [[ğŸŒ Website](https://socraticmodels.github.io/)] [[ğŸ’» Code](https://github.com/google-research/google-research/tree/master/socraticmodels)]
- **SayCan**, *Do As I Can, Not As I Say: Grounding Language in Robotic Affordances*, 2022.04. [[ğŸ“„ Paper](https://arxiv.org/abs/2204.01691)] [[ğŸŒ Website](https://say-can.github.io/)] [[ğŸ’» Code](https://github.com/google-research/google-research/tree/master/saycan)]
- **Inner Monologue**, *Inner Monologue: Embodied Reasoning through Planning with Language Models*, 2022.07, CoRL 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2207.05608)] [[ğŸŒ Website](https://innermonologue.github.io/)]
- **PaLM-E**, *PaLM-E: An Embodied Multimodal Language Model*, 2023.03, ICML 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2303.03378)] [[ğŸŒ Website](https://palm-e.github.io/)]
- **EmbodiedGPT**, *EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought*, 2023.05, NeurIPS 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2305.15021)] [[ğŸŒ Website](https://embodiedgpt.github.io/)] [[ğŸ’» Code](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)] [[ğŸ“Š Dataset](https://github.com/EmbodiedGPT/EgoCOT_Dataset)]
- **DoReMi**, *DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment*, 2023.07, IROS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2307.00329)] [[ğŸŒ Website](https://sites.google.com/view/doremi-paper)]
- **ViLa**, *Look Before You Leap: Unveiling the Power of  GPT-4V in Robotic Vision-Language Planning*, 2023.11, Workshop on Vision-Language Models for Navigation and Manipulation, ICRA 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2311.17842)] [[ğŸŒ Website](https://robot-vila.github.io/)]
- **3D-VLA**, *3D-VLA: A 3D Vision-Language-Action Generative World Model*, 2024.03, ICML 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.09631)] [[ğŸŒ Website](https://vis-www.cs.umass.edu/3dvla)] [[ğŸ’» Code](https://github.com/UMass-Embodied-AGI/3D-VLA)]
- **Bi-VLA**, *Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations*, 2024.05, SMC 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2405.06039)]
- **RoboMamba**, *RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation*, 2024.06, NeurIPS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2406.04339)] [[ğŸŒ Website](https://sites.google.com/view/robomamba-web)] [[ğŸ’» Code](https://github.com/lmzpai/roboMamba)]
- **ReplanVLM**, *ReplanVLM: Replanning Robotic Tasks with Visual Language Models*, 2024.07. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.21762v1)]
- **BUMBLE**, *BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation*, 2024.10, ICRA 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.06237)] [[ğŸŒ Website](https://robin-lab.cs.utexas.edu/BUMBLE/)] [[ğŸ’» Code](https://github.com/UT-Austin-RobIn/BUMBLE)]
- **ReflectVLM**, *Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation*, 2025.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.16707)] [[ğŸŒ Website](https://reflect-vlm.github.io/)] [[ğŸ’» Code](https://github.com/yunhaif/reflect-vlm)] [[ğŸ“Š Dataset](https://huggingface.co/datasets/yunhaif/ReflectVLM-data-expert)] [[ğŸ¤— Model](https://huggingface.co/collections/yunhaif/reflectvlm-67b95e4316ab2d5f71ad4b25)]
- **Hi Robot**, *Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models*, 2025.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.19417)] [[ğŸŒ Website](https://www.pi.website/research/hirobot)]
- **RoboBrain**, *RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete*, 2025.02, CVPR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.21257)] [[ğŸŒ Website](https://superrobobrain.github.io/)] [[ğŸ’» Code](https://github.com/FlagOpen/RoboBrain)] [[ğŸ“Š Dataset](https://huggingface.co/datasets/BAAI/ShareRobot)]
- **$\pi_{0.5}$**, $\pi_{0.5}$*: a Vision-Language-Action Model with Open-World Generalization*, 2025.04. [[ğŸ“„ Paper](https://arxiv.org/abs/2504.16054)] [[ğŸŒ Website](https://www.physicalintelligence.company/blog/pi05)]

## Language Motion

- **RT-H**, *RT-H: Action Hierarchies Using Language*, 2024.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.01823)] [[ğŸŒ Website](https://rt-hierarchy.github.io/)]
- **NaVILA**, *NaVILA: Legged Robot Vision-Language-Action Model for Navigation*, 2024.12, RSS 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2412.04453)] [[ğŸŒ Website](https://navila-bot.github.io/)] [[ğŸ’» Code](https://github.com/yang-zj1026/legged-loco)]

# Code as Action Tokens

- **Code as Policies**, *Code as Policies: Language Model Programs for Embodied Control*, 2022.09, ICRA 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2209.07753)] [[ğŸŒ Website](https://code-as-policies.github.io)] [[ğŸ’» Code](https://github.com/google-research/google-research/tree/master/code_as_policies)]
- **ProgPrompt**, *ProgPrompt: Generating Situated Robot Task Plans using Large Language Models*, 2022.09, ICRA 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2209.11302)] [[ğŸŒ Website](https://progprompt.github.io)] [[ğŸ’» Code](https://github.com/NVlabs/progprompt-vh)]
- **ChatGPT for Robotics**, *ChatGPT for Robotics: Design Principles and Model Abilities*, 2023.02, IEEE Access 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2306.17582)] [[ğŸŒ Website](https://www.microsoft.com/en-us/research/articles/chatgpt-for-robotics/)] [[ğŸ’» Code](https://github.com/microsoft/PromptCraft-Robotics)]
- **Text2Motion**, *Text2Motion: From Natural Language Instructions to Feasible Plans*, 2023.03, ICRL 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2303.12153)] [[ğŸŒ Website](https://sites.google.com/stanford.edu/text2motion)]
- **Instruct2Act**, *Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model*, 2023.05. [[ğŸ“„ Paper](https://arxiv.org/abs/2305.11176)] [[ğŸ’» Code](https://github.com/OpenGVLab/Instruct2Act)]
- **RoboScript**, *RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation*, 2024.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2402.14623)]
- **RoboCodeX**, *RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis*, 2024.02, ICML 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2402.16117)] [[ğŸŒ Website](https://sites.google.com/view/robocodexplus)] [[ğŸ’» Code](https://github.com/RoboCodeX-source/RoboCodeX_code)]

# Affordance as Action Tokens

## Keypoint

- **KITE**, *KITE: Keypoint-Conditioned Policies for Semantic Manipulation*, 2023.6, CoRL 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2306.16605)] [[ğŸŒ Website](https://sites.google.com/view/kite-website/home)] [[ğŸ’» Code](https://github.com/priyasundaresan/kite_keypoint_training)]
- **CoPa**, *CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models*, 2024.3, IROS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.08248)] [[ğŸŒ Website](https://copa-2024.github.io/)] [[ğŸ’» Code](https://github.com/HaoxuHuang/copa)]
- **RoboPoint**, *RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics*, 2024.6, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2406.10721)] [[ğŸŒ Website](https://robo-point.github.io/)] [[ğŸ’» Code](https://github.com/wentaoyuan/RoboPoint)]
- **RAM**, *RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation*, 2024.7, CoRL 2024 Oral. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.04689)] [[ğŸŒ Website](https://yuxuank.com/RAM/)] [[ğŸ’» Code](https://github.com/yxKryptonite/RAM_code)]
- **ReKep**, *ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation*, 2024.9, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2409.01652)] [[ğŸŒ Website](https://rekep-robot.github.io/)] [[ğŸ’» Code](https://github.com/huangwl18/ReKep)]
- **OmniManip**, *OmniManip: Towards General Robotic Manipulation via Object-Centric  Interaction Primitives as Spatial Constraints*, 2025.1, CVPR 2025 Highlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2501.03841)] [[ğŸŒ Website](https://omnimanip.github.io/)]
- **Magma**, *Magma: A Foundation Model for Multimodal AI Agents*, 2025.2, CVPR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.13130)] [[ğŸŒ Website](https://microsoft.github.io/Magma/)] [[ğŸ’» Code](https://github.com/microsoft/Magma)]
- **KUDA**, *KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation*, 2025.3, ICRA 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.10546)] [[ğŸŒ Website](https://kuda-dynamics.github.io/)] [[ğŸ’» Code](https://github.com/StoreBlank/KUDA)]

## Bounding Box

- **GPT-4V**, *GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration*, 2023.11, RA-L 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2311.12015)] [[ğŸŒ Website](https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/)] [[ğŸ’» Code](https://github.com/microsoft/GPT4Vision-Robot-Manipulation-Prompts)]
- **A3VLM**, *A3VLM: Actionable Articulation-Aware Vision Language Model*, 2024.6, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2406.07549)] [[ğŸ’» Code](https://github.com/changhaonan/A3VLM)]
- **DexGraspVLA**, *DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping*, 2025.2. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.20900)] [[ğŸŒ Website](https://dexgraspvla.github.io/)] [[ğŸ’» Code](https://github.com/Psi-Robot/DexGraspVLA)]

## Segmentation Mask

- **MOO**, *Open-World Object Manipulation using Pre-Trained Vision-Language Models*, 2023.3, CoRL 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2303.00905)] [[ğŸŒ Website](https://robot-moo.github.io/)]
- **ROCKET-1**, *ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting*, 2024.11, CVPR 2025. [[ğŸ“„ Paper](https://openaccess.thecvf.com/content/CVPR2025/html/Cai_ROCKET-1_Mastering_Open-World_Interaction_with_Visual-Temporal_Context_Prompting_CVPR_2025_paper.html)] [[ğŸŒ Website](https://craftjarvis.github.io/ROCKET-1)]
- **SoFar**, *SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation*, 2025.2. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.13143)] [[ğŸŒ Website](https://qizekun.github.io/sofar/)] [[ğŸ’» Code](https://github.com/qizekun/SoFar)]
- **RoboDexVLM**, *RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation*, 2025.3. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.01616)] [[ğŸŒ Website](https://henryhcliu.github.io/robodexvlm/)]

## Affordance Map

- **CLIPort**, *CLIPort: What and Where Pathways for Robotic Manipulation*, 2021.9, CoRL 2021. [[ğŸ“„ Paper](https://arxiv.org/abs/2109.12098)] [[ğŸŒ Website](https://cliport.github.io/)] [[ğŸ’» Code](https://github.com/cliport/cliport)]
- **VoxPoser**, *VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models*, 2023.7, CoRL 2023 Oral. [[ğŸ“„ Paper](https://arxiv.org/abs/2307.05973)] [[ğŸŒ Website](https://voxposer.github.io/)] [[ğŸ’» Code](https://github.com/huangwl18/VoxPoser)]
- **ManipLLM**, *ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation*, 2023.12, CVPR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2312.16217)] [[ğŸŒ Website](https://sites.google.com/view/manipllm)] [[ğŸ’» Code](https://github.com/clorislili/ManipLLM)]
- **ManiFoundation**, *ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots*, 2024.5, IROS 2024 Oral. [[ğŸ“„ Paper](https://arxiv.org/abs/2405.06964)] [[ğŸŒ Website](https://manifoundationmodel.github.io/)] [[ğŸ’» Code](https://github.com/NUS-LinS-Lab/ManiFM)]
- **MOKA**, *MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting*, 2024.5, RSS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.03174)] [[ğŸŒ Website](https://moka-manipulation.github.io/)] [[ğŸ’» Code](https://github.com/moka-manipulation/moka)]

# Trajectory as Action Tokens

## Robotic Manipulation

- **AVDC**, *Learning to Act from Actionless Videos through Dense Correspondences*, 2023.10, ICLR 2024 spotlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.08576)]
- **RT-Trajectory**, *RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches*, 2023.11, ICLR 2024 (Spotlight). [[ğŸ“„ Paper](https://arxiv.org/abs/2311.01977)] [[ğŸŒ Website](https://rt-trajectory.github.io)]
- **ATM**, *Any-point Trajectory Modeling for Policy Learning*, 2023.12, RSS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2401.00025)] [[ğŸŒ Website](https://xingyu-lin.github.io/atm/)] [[ğŸ’» Code](https://github.com/Large-Trajectory-Model/ATM)]
- **LLARVA**, *LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning*, 2024.06, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2406.11815)] [[ğŸŒ Website](https://llarva24.github.io)] [[ğŸ’» Code](https://github.com/Dantong88/LLARVA)] [[ğŸ“Š Dataset](https://github.com/Dantong88/LLARVA/blob/main/docs/DATASET.md)]
- **Im2Flow2Act**, *Flow as the Cross-Domain Manipulation Interface*, 2024.07, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.15208)] [[ğŸŒ Website](https://im-flow-act.github.io)] [[ğŸ’» Code](https://github.com/real-stanford/im2Flow2Act)]
- **FLIP**, *FLIP : Flow-Centric Generative Planning as General-Purpose Manipulation World Model*, 2024.12, ICLR 2025 Poster. [[ğŸ“„ Paper](https://arxiv.org/abs/2412.08261)] [[ğŸŒ Website](https://nus-lins-lab.github.io/flipweb/)] [[ğŸ’» Code](https://github.com/HeegerGao/FLIP)]
- **HAMSTER**, *HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation*, 2025.02, ICLR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.05485)] [[ğŸŒ Website](https://hamster-robot.github.io)]
- **ARM4R**, *Pre-training Auto-regressive Robotic Models with 4D Representations*, 2025.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.13142)]
- **Magma**, *Magma: A foundation model for multimodal AI agents*, 2025.02, CVPR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.13130)] [[ğŸŒ Website](https://microsoft.github.io/Magma/)] [[ğŸ’» Code](https://github.com/microsoft/Magma)] [[ğŸ¤— Model](https://huggingface.co/microsoft/Magma-8B)]

## Autonomous Driving

- **DriveVLM**, *DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models*, 2024.02, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2402.12289)] [[ğŸŒ Website](https://tsinghua-mars-lab.github.io/DriveVLM/)]
- **CoVLA**, *CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving*, 2024.08, WACV 2025 Oral. [[ğŸ“„ Paper](https://arxiv.org/abs/2408.10845)] [[ğŸŒ Website](https://turingmotors.github.io/covla-ad/)] [[ğŸ“Š Dataset](https://huggingface.co/datasets/turing-motors/CoVLA-Dataset)]
- **EMMA**, *EMMA: End-to-End Multimodal Model for Autonomous Driving*, 2024.10. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.23262)]
- **VLM-E2E**, *VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion*, 2025.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.18042)]

# Goal State as Action Tokens

## Single-Frame Image / Point Cloud

* **SuSIE**, *Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models*, 2023.10, ICLR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.10639)] [[ğŸŒ Website](https://rail-berkeley.github.io/susie/)] [[ğŸ’» Code](https://github.com/kvablack/susie)]
* **3D-VLA**, *3D-VLA: A 3D Vision-Language-Action Generative World Model*, 2024.03, ICML 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.09631)] [[ğŸŒ Website](https://vis-www.cs.umass.edu/3dvla/)]
* **CoTDiffusion**, *Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts*, 2024.06, CVPR 2024. [[ğŸ“„ Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Ni_Generate_Subgoal_Images_before_Act_Unlocking_the_Chain-of-Thought_Reasoning_in_CVPR_2024_paper.html)] [[ğŸŒ Website](https://cotdiffusion.github.io/)]
* **CoT-VLA**, *CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models*, 2025.03, CVPR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.22020)] [[ğŸŒ Website](https://cot-vla.github.io/)]

## Multi-Frame Video

* **UniPi**, *Learning Universal Policies via Text-Guided Video Generation*, 2023.02, NeurIPS 2023 spotlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2302.00111)] [[ğŸŒ Website](https://universal-policy.github.io/unipi/)]
* **AVDC**, *Learning to Act from Actionless Videos through Dense Correspondences*, 2023.10, ICLR 2024 spotlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.08576)] [[ğŸŒ Website](https://flow-diffusion.github.io/)] [[ğŸ’» Code](https://github.com/flow-diffusion/AVDC)]
* **VLP**, *Video Language Planning*, 2023.10. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.10625)] [[ğŸŒ Website](https://video-language-planning.github.io/)] [[ğŸ’» Code](https://github.com/video-language-planning/vlp_code)]
* **Gen2Act**, *Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation*, 2024.09, CoRL-X-Embodiment-WS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2409.16283)] [[ğŸŒ Website](https://homangab.github.io/gen2act/)]
* **Video Prediction Policy**, *Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations*, 2024.12, ICML 2025 Spotlight. [[ğŸ“„ Paper](https://arxiv.org/abs/2412.14803)] [[ğŸŒ Website](https://video-prediction-policy.github.io/)] [[ğŸ’» Code](https://github.com/roboterax/video-prediction-policy)]
* **FLIP**, *FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model*, 2024.12, ICLR 2025 [[ğŸ“„ Paper](https://arxiv.org/abs/2412.08261)] [[ğŸŒ Website](https://nus-lins-lab.github.io/flipweb/)] [[ğŸ’» Code](https://github.com/HeegerGao/FLIP)]
* **GEVRM**, *GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation*, 2025.02, ICLR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.09268)]

# Latent Representation as Action Tokens

- **OmniJARVIS**, *OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents*, 2024.07, NeurIPS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.00114)] [[ğŸŒ Website](https://omnijarvis.github.io)] [[ğŸ’» Code](https://github.com/CraftJarvis/OmniJarvis)] [[ğŸ“Š Dataset](https://huggingface.co/datasets/zhwang4ai/Minecraft-EmbodiedQA-300k)]
- **QueST**, *QueST: Self-Supervised Skill Abstractions for Learning Continuous Control*, 2024.07, NeurIPS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.15840)] [[ğŸŒ Website](https://quest-model.github.io)] [[ğŸ’» Code](https://github.com/pairlab/QueST)]
- **LAPA**, *LAPA: Latent Action Pretraining from Videos*, 2024.10, ICLR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.11758)] [[ğŸŒ Website](https://latentactionpretraining.github.io)] [[ğŸ’» Code](https://github.com/LatentActionPretraining/LAPA)] [[ğŸ¤— Model](https://huggingface.co/latent-action-pretraining/LAPA-7B-openx)]
- **GROOT-2**, *GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents*, 2024.12, ICLR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2412.10410)]
- **GO-1**, *AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.06669)] [[ğŸŒ Website](https://agibot-world.com)] [[ğŸ’» Code](https://github.com/OpenDriveLab/AgiBot-World)] [[ğŸ“Š Dataset](https://huggingface.co/agibot-world)]
- **UniVLA**, *UniVLA: Learning to Act Anywhere with Task-centric Latent Actions*, 2025.05, RSS2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2505.06111)] [[ğŸ’» Code](https://github.com/OpenDriveLab/UniVLA)] [[ğŸ¤— Model](https://github.com/OpenDriveLab/UniVLA#ckpts)]

# Raw Action as Action Tokens

- **LangLfP**, *Language-Conditioned Imitation Learning over Unstructured Data*, 2020.05, RSS 2021. [[ğŸ“„ Paper](https://arxiv.org/abs/2005.07648)] [[ğŸŒ Website](https://language-play.github.io)]
- **BC-Z**, *Zero-Shot Task Generalization with Robotic Imitation Learning*, 2022.02, CoRL 2021. [[ğŸ“„ Paper](https://arxiv.org/abs/2202.02005)] [[ğŸŒ Website](https://sites.google.com/view/bc-z/home?pli=1)] [[ğŸ’» Code](https://github.com/google-research/tensor2robot/tree/master/research/bcz)]
- **Gato**, *A Generalist Agent*, 2022.05, TMLR 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2205.06175)] [[ğŸŒ Website](https://deepmind.google/discover/blog/a-generalist-agent/)]
- **VIMA**, *VIMA: General Robot Manipulation with Multimodal Prompts*, 2022.10, ICML 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2210.03094)] [[ğŸŒ Website](https://vimalabs.github.io/)] [[ğŸ’» Code](https://github.com/vimalabs/VIMA)] [[ğŸ¤— Model](https://huggingface.co/VIMA/VIMA)]
- **RT-1**, *RT-1: Robotics Transformer for Real-World Control at Scale*, 2022.12, RSS 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2212.06817)] [[ğŸŒ Website](https://robotics-transformer1.github.io/)] [[ğŸ’» Code](https://github.com/google-research/robotics_transformer)]
- **RT-2**, *RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control*, 2023.07, CoRL 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2307.15818)] [[ğŸŒ Website](https://robotics-transformer2.github.io/)]
- **RT-X**, *Open X-Embodiment: Robotic Learning Datasets and RT-X Models*, 2023.10, ICRA 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.08864)] [[ğŸŒ Website](https://robotics-transformer-x.github.io)] [[ğŸ’» Code](https://github.com/google-deepmind/open_x_embodiment)]
- **RoboFlamingo**, *Vision-Language Foundation Models as Effective Robot Imitators*, 2023.11, ICLR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2311.01378)] [[ğŸŒ Website](https://roboflamingo.github.io)] [[ğŸ’» Code](https://github.com/RoboFlamingo/RoboFlamingo)] [[ğŸ¤— Model](https://huggingface.co/roboflamingo/RoboFlamingo)]
- **LEO**, *An Embodied Generalist Agent in 3D World*, 2023.11, ICML 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2311.12871)] [[ğŸŒ Website](https://embodied-generalist.github.io/)] [[ğŸ’» Code](https://github.com/embodied-generalist/embodied-generalist?tab=readme-ov-file)]
- **GR-1**, *Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation*, 2023.12, ICLR 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2312.13139)] [[ğŸŒ Website](https://gr1-manipulation.github.io/)] [[ğŸ’» Code](https://github.com/bytedance/GR-1)]
- **Octo**, *Octo: An Open-Source Generalist Robot Policy*, 2024.05, ICRA 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2405.12213)] [[ğŸŒ Website](https://octo-models.github.io)] [[ğŸ’» Code](https://github.com/octo-models/octo)] [[ğŸ¤— Model](https://huggingface.co/rail-berkeley)]
- **OpenVLA**, *OpenVLA: An open-source vision-language-action model*, 2024.06, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2406.09246)] [[ğŸŒ Website](https://openvla.github.io/)] [[ğŸ’» Code](https://github.com/openvla/openvla)] [[ğŸ¤— Model](https://huggingface.co/openvla)]
- **TinyVLA**, *TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation*, 2024.09, RA-L 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2409.12514)] [[ğŸŒ Website](https://tiny-vla.github.io/)] [[ğŸ’» Code](https://github.com/liyaxuanliyaxuan/TinyVLA)]
- **HiRT**, *HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers*, 2024.09, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.05273)]
- **GR-2**, *GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation*, 2024.10. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.06158v1)] [[ğŸŒ Website](https://gr2-manipulation.github.io/)]
- **RDT-1B**, *RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation*, 2024.10, ICLR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.07864)] [[ğŸŒ Website](https://rdt-robotics.github.io/rdt-robotics/)] [[ğŸ’» Code](https://rdt-robotics.github.io/rdt-robotics/)] [[ğŸ¤— Model](https://huggingface.co/robotics-diffusion-transformer/rdt-1b)]
- **$\pi_0$**, *A Vision-Language-Action Flow Model for General Robot Control*, 2024.10. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.24164)] [[ğŸŒ Website](https://www.physicalintelligence.company/blog/pi0)] [[ğŸ¤— Model](https://huggingface.co/lerobot/pi0)]
- **CogACT**, *CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation*, 2024.11. [[ğŸ“„ Paper](https://arxiv.org/abs/2411.19650)] [[ğŸŒ Website](https://cogact.github.io/)] [[ğŸ’» Code](https://github.com/microsoft/CogACT?tab=readme-ov-file)] [[ğŸ¤— Model](https://huggingface.co/CogACT/CogACT-Base)]
- **$\pi_0$-FAST**, *FAST: Efficient Action Tokenization for Vision-Language-Action Models*, 2025.01. [[ğŸ“„ Paper](https://arxiv.org/abs/2501.09747)] [[ğŸŒ Website](https://www.pi.website/research/fast)] [[ğŸ¤— Model](https://huggingface.co/lerobot/pi0fast_base)]
- **UniAct**, *Universal Actions for Enhanced Embodied Foundation Models*, 2025.01, CVPR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2501.10105)] [[ğŸŒ Website](https://2toinf.github.io/UniAct/)] [[ğŸ’» Code](https://github.com/2toinf/UniAct)]
- **OpenVLA-OFT**, *Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success*, 2025.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.19645)] [[ğŸŒ Website](https://openvla-oft.github.io)] [[ğŸ’» Code](https://github.com/moojink/openvla-oft)]
- **JARVIS-VLA**, *Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.16365)] [[ğŸŒ Website](https://craftjarvis.github.io/JarvisVLA/)] [[ğŸ’» Code](https://github.com/CraftJarvis/JarvisVLA)] [[ğŸ¤— Model](https://huggingface.co/collections/CraftJarvis/jarvis-vla-v1-67dc157a99d011efd7d7f7e4)]
- **HybridVLA**, *HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.10631)] [[ğŸŒ Website](https://hybrid-vla.github.io)] [[ğŸ’» Code](https://github.com/PKU-HMI-Lab/Hybrid-VLA)]
- **MoManipVLA**, *MoManipVLA: Transferring Vision-Language-Action Models for General Mobile Manipulation*, 2025.03, CVPR 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.13446)]
- **GR00T N1**, *GR00T N1: An Open Foundation Model for Generalist Humanoid Robots*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.14734)] [[ğŸŒ Website](https://developer.nvidia.com/isaac/gr00t)] [[ğŸ’» Code](https://github.com/NVIDIA/Isaac-GR00T)] [[ğŸ¤— Model](https://huggingface.co/nvidia/GR00T-N1.5-3B)]
- **$\pi_0$+KI**, *Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better*, 2025.05. [[ğŸ“„ Paper](https://arxiv.org/abs/2505.23705)] [[ğŸŒ Website](https://www.physicalintelligence.company/research/knowledge_insulation)]
- **RTC**, *Real-Time Execution of Action Chunking Flow Policies*, 2025.06. [[ğŸ“„ Paper](https://arxiv.org/abs/2506.07339)] [[ğŸŒ Website](https://www.pi.website/research/real_time_chunking)]

# Reasoning as Action Tokens

- **Inner Monologue**, *Inner Monologue: Embodied Reasoning through Planning with Language Models*, 2022.07, CoRL 2022. [[ğŸ“„ Paper](https://arxiv.org/abs/2207.05608)] [[ğŸŒ Website](https://innermonologue.github.io/)]
- **DriveVLM**, *DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models*, 2024.02, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2402.12289)] [[ğŸŒ Website](https://tsinghua-mars-lab.github.io/DriveVLM/)]
- **ECoT**, *Robotic Control via Embodied Chain-of-Thought Reasoning*, 2024.07, CoRL 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.08693)] [[ğŸŒ Website](https://embodied-cot.github.io/)]
- **RAD**, *Action-Free Reasoning for Policy Generalization*, 2025.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.03729)] [[ğŸŒ Website](https://rad-generalization.github.io/)]
- **AlphaDrive**, *AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.07608)] [[ğŸŒ Website](https://github.com/hustvl/AlphaDrive)]
- **Cosmos-Reason1**, *Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.15558)] [[ğŸŒ Website](https://research.nvidia.com/labs/dir/cosmos-reason1/)] [[ğŸ’» Code](https://github.com/nvidia-cosmos/cosmos-reason1?tab=readme-ov-file)]

# Scalable Data Sources

## Bottom Layer: Web Data and Human Video

- **Something-Something V2**, *The" something something" video database for learning and evaluating visual common sense*, 2017.06. [[ğŸ“„ Paper](https://arxiv.org/abs/1706.04261)] [[ğŸŒ Website](https://www.qualcomm.com/developer/software/something-something-v-2-dataset)]
- **EPIC-KITCHENS-100**, *Scaling Egocentric Vision: The EPIC-KITCHENS Dataset*, 2018.04. [[ğŸ“„ Paper](https://arxiv.org/abs/1804.02748)] [[ğŸŒ Website](https://epic-kitchens.github.io/2020-100)]
- **Ego4D**, *Ego4D: Around the World in 3,000 Hours of Egocentric Video*, 2021.10. [[ğŸ“„ Paper](https://arxiv.org/abs/2110.07058)] [[ğŸŒ Website](https://ego4d-data.org)]
- **Ego-Exo4D**, *Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives*, 2023.11. [[ğŸ“„ Paper](https://arxiv.org/abs/2311.18259)] [[ğŸŒ Website](https://ego-exo4d-data.org/)]

## Middle Layer: Synthetic and Simulation Data

- **MimicGen**, *MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations*, 2023.10, CoRL 2023. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.17596)]
- **RoboCase**, *RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots*, 2024.06, RSS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2406.02523)] [[ğŸŒ Website](https://robocasa.ai/)] [[ğŸ’» Code](https://github.com/robocasa/robocasa)]
- **DexMimicGen**, *DexMimicGen: Automated Data Generation for  Bimanual Dexterous Manipulation via Imitation Learning*, 2024.10, ICRA 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2410.24185)] [[ğŸŒ Website](https://dexmimicgen.github.io)]
- **AgiBot DigitalWorld**, *AgiBot DigitalWorld*, 2025.02. [[ğŸŒ Website](https://agibot-digitalworld.com/)]

## Top Layer: Real-world Robot Data

- **nuScenes**, *nuScenes: A multimodal dataset for autonomous driving*, 2019.03, CVPR 2020. [[ğŸ“„ Paper](https://arxiv.org/abs/1903.11027)] [[ğŸŒ Website](https://www.nuscenes.org/)] [[ğŸ’» Code](https://github.com/nutonomy/nuscenes-devkit)]
- **WOMD**, *Large Scale Interactive Motion Forecasting for Autonomous Driving: The Waymo Open Motion Dataset*, 2021.04, ICCV 2021. [[ğŸ“„ Paper](https://arxiv.org/abs/2104.10133)] [[ğŸŒ Website](https://waymo.com/open/)] [[ğŸ’» Code](https://github.com/waymo-research/waymo-open-dataset)]
- **RT-1**, *RT-1: Robotics Transformer for Real-World Control at Scale*, 2022.12. [[ğŸ“„ Paper](https://arxiv.org/abs/2212.06817)] [[ğŸŒ Website](https://robotics-transformer1.github.io/)]
- **RH20T**, *RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot*, 2023.06, ICRA 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2307.00595)] [[ğŸŒ Website](https://rh20t.github.io)]
- **BridgeData V2**, *BridgeData V2: A Dataset for Robot Learning at Scale*, 2023.08, CoRL 2o23. [[ğŸ“„ Paper](https://arxiv.org/abs/2308.12952)] [[ğŸŒ Website](https://rail-berkeley.github.io/bridgedata/)]
- **OXE**, *Open X-Embodiment: Robotic Learning Datasets and RT-X Models*, 2023.10, ICRA 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2310.08864)] [[ğŸŒ Website](https://robotics-transformer-x.github.io)]
- **HoNY**, *On Bringing Robots Home*, 2023.11. [[ğŸ“„ Paper](https://arxiv.org/abs/2311.16098)] [[ğŸŒ Website](https://dobb-e.com)]
- **DROID**, *DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset*, 2024.03, RSS 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2403.12945)] [[ğŸŒ Website](https://huggingface.co/KarlP/droid)]
- **CoVLA**, *CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving*, 2024.08, WACV 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2408.10845)] [[ğŸŒ Website](https://turingmotors.github.io/covla-ad/)]
- **RoboMIND**, *RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation*, 2024.12, RSS 2025. [[ğŸ“„ Paper](https://arxiv.org/abs/2412.13877)] [[ğŸŒ Website](https://x-humanoid-robomind.github.io/)]
- **AgiBot World**, *AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.06669)] [[ğŸŒ Website](https://agibot-world.com/)] [[ğŸ’» Code](https://github.com/OpenDriveLab/AgiBot-World)]

# Related Surveys

- *Robot Learning in the Era of Foundation Models: A Survey*, 2023.11, Neurocomputing Volume 638. [[ğŸ“„ Paper](https://arxiv.org/abs/2311.14379)]
- *A Survey on Robotics with Foundation Models: toward Embodied AI*, 2024.02. [[ğŸ“„ Paper](https://arxiv.org/abs/2402.02385)]
- *A Survey on Integration of Large Language Models with Intelligent Robots*, 2024.04, Intelligent Service Robotics 2024. [[ğŸ“„ Paper](https://arxiv.org/abs/2404.09228)]
- *What Foundation Models can Bring for Robot Learning in Manipulation: A Survey*, 2024.04. [[ğŸ“„ Paper](https://arxiv.org/abs/2404.18201)]
- *A Survey on Vision-Language-Action Models for Embodied AI*, 2024.05. [[ğŸ“„ Paper](https://arxiv.org/abs/2405.14093)]
- *Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI*, 2024.07. [[ğŸ“„ Paper](https://arxiv.org/abs/2407.06886)]
- *Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions*, 2025.02, Information Fusion Volume 122. [[ğŸ“„ Paper](https://arxiv.org/abs/2502.15336)]
- *Generative Artificial Intelligence in Robotic Manipulation: A Survey*, 2025.03. [[ğŸ“„ Paper](https://arxiv.org/abs/2503.03464)]
- *OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation*, 2025.05. [[ğŸ“„ Paper](https://arxiv.org/abs/2505.03912)]
- *Vision-Language-Action Models: Concepts, Progress, Applications and Challenges*, 2025.05. [[ğŸ“„ Paper](https://arxiv.org/abs/2505.04769)]
